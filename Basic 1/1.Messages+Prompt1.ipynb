{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PasteOpenAIKeyHere'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# install the python-dotenv module\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() \n",
    "# this looks for a file called `.env` in your project directory\n",
    "# finds and  loads variables from `.env` into the environment variable space\n",
    "\n",
    "\n",
    "import os\n",
    "openai_key = os.environ.get('OPEN_AI_KEY')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain Introduction\n",
    "\n",
    "1. There are two main types of models that Langchain integrates with LLMs and Chat Models.\n",
    "\n",
    "2. LLMs ->  text completion model, API wrap a string prompt as input and output a string completion.\n",
    "\n",
    "3. ChatModels -> backed by LLms but tuned specifically for conversation. Instead of single string they take -> list of chat messages (a thread). Provider API use a different interface than pure text completion model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Messages\n",
    "\n",
    "* ChatModels take a list of messages as input and return a message.\n",
    "    * All messages have a `role` (describes who is saying this message, LangChain has different message class for different roles)  and a `content`(can be string or list of dictionary ) property.\n",
    "\n",
    "    \n",
    "    * `SystemMessage` : tells the model how to behave, only some model supports this.\n",
    "\n",
    "    * `FuntionMessage` : represents the result of function call, addition to role and content -> this message has a name parameter -> which conveys the name of the function that was called to produce this result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMs\n",
    "\n",
    "1. provides interface to interact with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI( openai_api_key = openai_key)\n",
    "\n",
    "response = llm.invoke(\"Who is Prime Minister of India ? \")\n",
    "# invoke funtion  takes a prompt and returns the completion response\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatModels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(openai_api_key = openai_key)\n",
    "response =  chat.invoke(\"Who is Prime Minister of India ? \")\n",
    "print(response)\n",
    "print(response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(openai_api_key = openai_key)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content = \"You're a helpful assistant\"), # behaviour of the model\n",
    "    HumanMessage(content = \"What is the purpose of model regularization ? \")\n",
    "]\n",
    "\n",
    "response =  chat.invoke(messages)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PromptTemplate\n",
    "\n",
    "1. What is a prompt ? \n",
    "* A prompt for a language model is a set of instructions or input provided by a user to guide the model response,\n",
    "* helping it understand the context and generate relevant and coherent language based output,\n",
    "* such as QnA system, completing sentence or engaging in a conversation\n",
    "\n",
    "2. What is a Prompt template ? \n",
    "* are predefined recipes for generating prompts for langauge model.\n",
    "* template may include instructions, few short examples and specific content and questions \n",
    "* string + dynamic value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "llm  = OpenAI(openai_api_key = openai_key)\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "noInputVariable = PromptTemplate(\n",
    "    input_variables= [],\n",
    "    template = \"Tell me a Python trick\"\n",
    ")\n",
    "\n",
    "noInput2 = PromptTemplate.from_template(\"Tell me a Python trick\")\n",
    "\n",
    "print(noInputVariable)\n",
    "print(noInput2)\n",
    "\n",
    "noInput2 == noInputVariable\n",
    "\n",
    "formattedNoInputPrompt = noInputVariable.format()\n",
    "print(formattedNoInputPrompt) # this can be directly sent to the LLM -> contains your entire prompt as a string\n",
    "\n",
    "response = llm.invoke(formattedNoInputPrompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['language'] template='Tell me a {language} Trick'\n",
      "input_variables=['language'] template='Tell me a {language} Trick'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One Input Variable\n",
    "\n",
    "oneInput = PromptTemplate(\n",
    "    input_variables=[\"language\"], \n",
    "    template = \"Tell me a {language} Trick\"\n",
    ")\n",
    "\n",
    "oneInput2 = PromptTemplate.from_template(\"Tell me a {language} Trick\")\n",
    "\n",
    "print(oneInput)\n",
    "print(oneInput2)\n",
    "\n",
    "oneInput2 == oneInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formattedOneInputPrompt = oneInput.format(language =\"C Programming\")\n",
    "formattedOneInputPrompt2 = oneInput2.format(language =\"C Programming\")\n",
    "\n",
    "\n",
    "formattedOneInputPrompt == formattedOneInputPrompt2\n",
    "\n",
    "print(formattedOneInputPrompt2)\n",
    "\n",
    "\n",
    "\n",
    "reponse = llm.invoke(formattedOneInputPrompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "input_variables=['language', 'tool'] template='Give me {language} trick related to {tool}'\n",
      "input_variables=['language', 'tool'] template='Give me {language} trick related to {tool}'\n"
     ]
    }
   ],
   "source": [
    "# Multiple Input variable\n",
    "multipleInput = PromptTemplate(\n",
    "    input_variables=['language','tool'],\n",
    "    template = \"Give me {language} trick related to {tool}\"\n",
    ")\n",
    "\n",
    "multiple2 = PromptTemplate.from_template('Give me {language} trick related to {tool}')\n",
    "\n",
    "print(multiple2 == multipleInput)\n",
    "print(multipleInput)\n",
    "print(multiple2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Give me Python trick related to Data Science\n"
     ]
    }
   ],
   "source": [
    "formatted_mul = multipleInput.format(language = \"Python\", tool = \"Data Science\")\n",
    "formatted_mul2 = multiple2.format(language = \"Python\", tool = \"Data Science\")\n",
    "\n",
    "print(formatted_mul == formatted_mul2)\n",
    "print(formatted_mul2)\n",
    "\n",
    "llm = OpenAI(openai_api_key = openai_key)\n",
    "\n",
    "response =  llm.invoke(formatted_mul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatModel PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PasteOpenAIKeyHere\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "openai_key = os.environ.get('OPEN_AI_KEY')\n",
    "print(openai_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input_language', 'output_language', 'text'] messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input_language', 'output_language'], template='You are a helpful assistant that translates {input_language} to {output_language}')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='{text}'))]\n",
      "Input variables are :  ['input_language', 'output_language', 'text']\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, PromptTemplate\n",
    "chatPrompt1 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])\n",
    "\n",
    "print(chatPrompt1)\n",
    "print(\"Input variables are : \",chatPrompt1.input_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful assistant that translates English to Hindi'), HumanMessage(content='How can i write this in english')]\n"
     ]
    }
   ],
   "source": [
    "formattedChatPrompt = chatPrompt1.format_messages(\n",
    "    input_language = \"English\",\n",
    "    output_language = \"Hindi\",\n",
    "    text = \"How can i write this in english\"\n",
    ")\n",
    "print(formattedChatPrompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(formattedChatPrompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Message Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant that translates English to Hindi'),\n",
       " HumanMessage(content='How can i write this in english')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_template  = \"You are a helpful assistant that translates {input_language} to {output_language}\"\n",
    "human_template =\"{text}\"\n",
    "\n",
    "chatPrompt2 = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(sys_template),\n",
    "    HumanMessagePromptTemplate.from_template(human_template)\n",
    "])\n",
    "\n",
    "chatPrompt1 == chatPrompt2\n",
    "\n",
    "formattedChatPrompt2 = chatPrompt2.format_messages(\n",
    "    input_language = \"English\",\n",
    "    output_language = \"Hindi\",\n",
    "    text = \"How can i write this in english\"\n",
    ")\n",
    "print(formattedChatPrompt2)\n",
    "\n",
    "response = chat.invoke(formattedChatPrompt2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input_language', 'output_language', 'text'] messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input_language', 'output_language'], template='You are a helpful assistant that translates {input_language} to {output_language}.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='{text}'))]\n",
      "['input_language', 'output_language', 'text']\n",
      "[SystemMessage(content='You are a helpful assistant that translates English to Hindi.'), HumanMessage(content='How can i write this in english')]\n"
     ]
    }
   ],
   "source": [
    "systemPrompt = PromptTemplate(\n",
    "    input_variables=['input_language','output_language'],\n",
    "    template ='You are a helpful assistant that translates {input_language} to {output_language}.'\n",
    ")\n",
    "humanPrompt = PromptTemplate.from_template('{text}')\n",
    "\n",
    "systemMessagePrompt = SystemMessagePromptTemplate(prompt=systemPrompt)\n",
    "humanMessgaePrompt = HumanMessagePromptTemplate(prompt = humanPrompt)\n",
    "\n",
    "chatPrompt = ChatPromptTemplate.from_messages([\n",
    "    systemMessagePrompt, humanMessgaePrompt\n",
    "])\n",
    "formattedChatPrompt2 = chatPrompt.format_messages(\n",
    "    input_language = \"English\",\n",
    "    output_language = \"Hindi\",\n",
    "    text = \"How can i write this in english\"\n",
    ")\n",
    "print(chatPrompt)\n",
    "print(chatPrompt.input_variables)\n",
    "print(formattedChatPrompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input_language', 'output_language'] template='You are a helpful assistant that translates {input_language} to {output_language}.'\n"
     ]
    }
   ],
   "source": [
    "systemPrompt = PromptTemplate.from_template(\n",
    "    'You are a helpful assistant that translates {input_language} to {output_language}.'\n",
    ")\n",
    "\n",
    "print(systemPrompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
